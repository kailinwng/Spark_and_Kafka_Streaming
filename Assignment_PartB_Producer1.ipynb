{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Management and Processing with Apache Spark & Kafka\n",
    "***\n",
    "\n",
    "Name: Wong Kai Lin\n",
    "***\n",
    "\n",
    "\n",
    "## Processing Data Stream \n",
    "### Event Producer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program that loads all the data from climate_streaming.csv and \n",
    "randomly (with replacement) feed the data to the stream every 10 seconds. \n",
    "\n",
    "You will need to append additional information \n",
    "such as producer information to identify the producer and \n",
    "created date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve last date from MongoDb collection\n",
    "\n",
    "def last_date():\n",
    "    \n",
    "    # Making a Connection with MongoDB\n",
    "    client = MongoClient()\n",
    "\n",
    "    # connecting to database\n",
    "    db = client.fit3182_assignment_db\n",
    "    # connecting to collection\n",
    "    climate_hotspot = db.climate_hotspot\n",
    "\n",
    "    # accesing the last date stored in the collection\n",
    "    last_date = list(climate_hotspot.find({},{'date':1,'_id':0}).sort('date',-1).limit(1)) # store py cursor object into list\n",
    "    last_date = last_date[0]['date'] # access the date value\n",
    "    last_date = dt.datetime.strptime(last_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    return(last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_message(producer_instance, topic_name, data):\n",
    "    \n",
    "    try:    \n",
    "\n",
    "        data = json.dumps(data) # dictionary data of climate converted to json string             \n",
    "\n",
    "        value_bytes = bytes(data, encoding='utf-8') # encode json string data to bytes\n",
    "\n",
    "        key_bytes = bytes(\"P1\", encoding='utf-8') # specific key for each producer\n",
    "\n",
    "        producer_instance.send(topic_name, value=value_bytes, key=key_bytes)\n",
    "        \n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Message published successfully. Data: {\"date\": \"2019-01-01\", \"climate\": {\"windspeed_knots\": 8.5, \"precipitation \": \" 0.08G\", \"air_temperature_celcius\": 13, \"longitude\": 143.7132, \"GHI\": 111, \"relative_humidity\": 50.1, \"latitude\": -36.369, \"max_wind_speed\": 12.0}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-02\", \"climate\": {\"windspeed_knots\": 4.1, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 21, \"longitude\": 143.281, \"GHI\": 176, \"relative_humidity\": 52.8, \"latitude\": -36.94, \"max_wind_speed\": 11.1}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-03\", \"climate\": {\"windspeed_knots\": 7.7, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 17, \"longitude\": 143.1847, \"GHI\": 143, \"relative_humidity\": 52.5, \"latitude\": -36.0005, \"max_wind_speed\": 16.9}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-04\", \"climate\": {\"windspeed_knots\": 5.7, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 18, \"longitude\": 149.30200000000002, \"GHI\": 149, \"relative_humidity\": 54.4, \"latitude\": -37.605, \"max_wind_speed\": 9.9}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-05\", \"climate\": {\"windspeed_knots\": 14.5, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 10, \"longitude\": 148.099, \"GHI\": 96, \"relative_humidity\": 35.6, \"latitude\": -37.333, \"max_wind_speed\": 20.0}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-06\", \"climate\": {\"windspeed_knots\": 12.2, \"precipitation \": \" 0.04G\", \"air_temperature_celcius\": 25, \"longitude\": 149.316, \"GHI\": 199, \"relative_humidity\": 58.3, \"latitude\": -37.583, \"max_wind_speed\": 22.0}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-07\", \"climate\": {\"windspeed_knots\": 7.2, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 10, \"longitude\": 147.167, \"GHI\": 90, \"relative_humidity\": 43.7, \"latitude\": -38.226, \"max_wind_speed\": 11.1}, \"producer\": 1}\n",
      "Message published successfully. Data: {\"date\": \"2019-01-08\", \"climate\": {\"windspeed_knots\": 3.1, \"precipitation \": \" 0.00I\", \"air_temperature_celcius\": 7, \"longitude\": 148.05, \"GHI\": 65, \"relative_humidity\": 40.5, \"latitude\": -37.562, \"max_wind_speed\": 8.0}, \"producer\": 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-978069f8cf03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpublish_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# feed data to spark streaming every 10 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'climate'\n",
    "    \n",
    "    print('Publishing records..')\n",
    "    producer1 = connect_kafka_producer()\n",
    "    \n",
    "    # reading csv\n",
    "    climate_csv = pd.read_csv(\"climate_streaming.csv\")\n",
    "    \n",
    "    # rename GHI column header\n",
    "    climate_csv.columns = ['latitude', 'longitude', 'air_temperature_celcius', 'relative_humidity', 'windspeed_knots', 'max_wind_speed', 'precipitation ', 'GHI']\n",
    "    head = list(climate_csv.columns) # column names\n",
    "\n",
    "    # getting last date stored in climate data of Part A\n",
    "    new_date = last_date()\n",
    "\n",
    "    while True:\n",
    "        row = random.randrange(0, len(climate_csv)-1) # retrieving a random row data from climate_csv\n",
    "               \n",
    "        climate_json = {} # json to store the row of climate data\n",
    "        \n",
    "        for cols in head:\n",
    "            climate = climate_csv.loc[row,cols] #value of each column of the data row            \n",
    "            climate_json[cols] = climate # append column name and data value into json\n",
    "\n",
    "        # convert numpy.int64 column values to int for JSON\n",
    "        climate_json['GHI'] = int(climate_json['GHI'])\n",
    "        climate_json['air_temperature_celcius'] = int(climate_json['air_temperature_celcius'])   \n",
    "            \n",
    "        # new date + 1 for every 10second data\n",
    "        new_date+=dt.timedelta(days=1)\n",
    "        new_date_str = (new_date.date()).isoformat()\n",
    "        \n",
    "        data = {'date': new_date_str, 'producer': 1, 'climate': climate_json} # creating json data\n",
    "\n",
    "        publish_message(producer1, topic, data)\n",
    "        \n",
    "        sleep(10) # feed data to spark streaming every 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
