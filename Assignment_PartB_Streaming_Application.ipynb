{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT3182 Big data management and processing - S1 2021 Assignment\n",
    "***\n",
    "\n",
    "Name: Wong Kai Lin\n",
    "\n",
    "Student ID: 30507588\n",
    "\n",
    "Email: kwon0061@student.monash.edu\n",
    "***\n",
    "\n",
    "\n",
    "## Part B- \n",
    "## Task 1. Processing Data Stream \n",
    "### d. Streaming Application:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a streaming application in Apache Spark Streaming \n",
    "which has a local streaming context with two execution threads and a batch interval \n",
    "of 10 seconds. The streaming application will receive streaming data from all three \n",
    "producers. If the streaming application has data from all or at least two of the \n",
    "producers, do the processing as follows:\n",
    "\n",
    "- Group the streams based on the location (i,e, latitude and longitude) and \n",
    "create the data model developed in Part A.\n",
    "\n",
    "- You can find if two locations are close to each other or not by implementing \n",
    "the geo-hashing algorithm or find a library that does the job for you. The \n",
    "precision number in the algorithm determines the number of characters in the \n",
    "Geohash. Please use precision 3. If the climate data and hotspot data are not \n",
    "close to each other we can ignore the hotspot data and just store the climate \n",
    "data.\n",
    "\n",
    "- If the streaming application has the data from only one producer (Producer \n",
    "1), it implies that there was no fire at that time and we can store the climate \n",
    "data into MongoDB straight away. \n",
    "\n",
    "- If we receive the data from two different satellites AQUA and TERRA for the \n",
    "same location (to determine whether the two locations are the same or not \n",
    "please use geohash with precision 5), then average the ‘surface temperature’ \n",
    "and ‘confidence’ from the two satellites and save it as a fire event.\n",
    "\n",
    "- If a fire was detected with an air temperature greater than 20 (°C) and a GHI \n",
    "greater than 180 (W/m2\n",
    "), then report the cause of the fire event as ‘natural’. \n",
    "Otherwise, report the cause of the fire event as ‘other’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awaiting message from producer...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd2944a4cc53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Awaiting message from producer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 10 minutes just in case no detection of producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;31m# ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pygeohash as pgh\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "# returns whether 2 locations are close to one another\n",
    "def closeness(lat1, lon1, lat2, lon2, prec):\n",
    "    hash_a = pgh.encode(lat1, lon1, precision= prec)\n",
    "    hash_b = pgh.encode(lat2, lon2, precision= prec)\n",
    "    \n",
    "    if hash_a == hash_b:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "\n",
    "    # iter - the 10 second data stream (list of messages from the (10secs) batch of data)\n",
    "    \n",
    "    # First part. Store the message into local variable\n",
    "    climate_data = {}\n",
    "    aqua_data = []\n",
    "    terra_data = []\n",
    "    \n",
    "    for record in iter:\n",
    "        \n",
    "        key = record[0]\n",
    "        \n",
    "        if key == \"P1\": # producer 1: climate data\n",
    "            climate = json.loads(record[1])\n",
    "            \n",
    "        elif key == \"P2\": # producer 2: hotspot_aqua data\n",
    "            aqua_data.append(json.loads(record[1]))\n",
    "            \n",
    "        elif key == \"P3\": # producer 3: hotspot_terra data\n",
    "            terra_data.append(json.loads(record[1]))\n",
    "    \n",
    "    \n",
    "    # Second part. Data processing  \n",
    "    \n",
    "    if climate_data == {}: # end the process if there is no climate data\n",
    "        print(\"There is no Climate data.\")\n",
    "        return\n",
    "        \n",
    "    #else:      \n",
    "    \n",
    "        # 1. Compare climate and each hotspot's location with precision=3\n",
    "            # If not close...\n",
    "                # Remove the data\n",
    "                \n",
    "        #hotspot = []\n",
    "        #fire_event = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # 2. Combine the hotspot data that are close to each other\n",
    "\n",
    "            # EXAMPLES:\n",
    "            # 2 hospot aqua and 1 hotspot terra produce same geohash, combine 3 to 1\n",
    "                # average the surface temperature and confidence\n",
    "                # average lat, lon\n",
    "                # date follow climate, time choose any one of the data\n",
    "\n",
    "            # Hint: for....loop , hash parition, merge (group operation for data with same attribute)\n",
    "            \n",
    "            \n",
    "        # 3. Decide the fire event is natural or other based on the air temparature and WHI (in climate data)\n",
    "             \n",
    "            # Get the air temparature and WHI from climate\n",
    "            \n",
    "            # Append the 'natural' or 'other' to the hostpot data\n",
    "                # Create a new field to store the value\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Last part. Store the data into mongodb\n",
    "    client = MongoClient()\n",
    "    db = client.fit3182_assignment_db # database\n",
    "    streaming = db.streaming_data # create new collection\n",
    "    \n",
    "    #json_data = {'date': date, 'climate': climate_data, 'hotspot': hotspot, 'fire_event': fire_event}\n",
    "    \n",
    "    for record in iter:           \n",
    "        data = record[1].split(\":\")\n",
    "        key = record[0]\n",
    "\n",
    "        json_data = {}\n",
    "\n",
    "        json_data[\"producer\"] = key # data from which producer\n",
    "                \n",
    "        try:\n",
    "            streaming.insert(json_data) # insert data into MongoDb\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                \n",
    "    client.close()\n",
    "\n",
    "n_secs = 10\n",
    "topic = \"hotspot\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "# Direct stream to receive data from Kafka - Establish connection    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'localhost:9092', \n",
    "                        'group.id':'streaming_data', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary \n",
    "\n",
    "# send every partition of data in each batch of data (data every 10secs) into sendDataToDB()         \n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    " \n",
    "ssc.start()\n",
    "print('Awaiting message from producer...')\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)\n",
    "print('Process stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
